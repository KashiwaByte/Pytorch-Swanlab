1 [SwanLab-INFO]:        Experiment resnet_inference_Mar07_10-48-50 has been registered.
2 [SwanLab-INFO]:        Run data will be saved locally in D:\Pytorch-Swanlab\hello_pytorch\lesson\lesson-32\swanlog\run-20240307_104850-0b2fe887
3 [SwanLab-INFO]:        Experiment_name: resnet_inference_Mar07_10-48-50
4 [SwanLab-INFO]:        Run `swanlab watch` to view SwanLab Experiment Dashboard
5 ----------------------------------------------------------------
6         Layer (type)               Output Shape         Param #
7 ================================================================
8             Conv2d-1         [-1, 64, 112, 112]           9,408
9        BatchNorm2d-2         [-1, 64, 112, 112]             128
10               ReLU-3         [-1, 64, 112, 112]               0
11          MaxPool2d-4           [-1, 64, 56, 56]               0
12             Conv2d-5           [-1, 64, 56, 56]          36,864
13        BatchNorm2d-6           [-1, 64, 56, 56]             128
14               ReLU-7           [-1, 64, 56, 56]               0
15             Conv2d-8           [-1, 64, 56, 56]          36,864
16        BatchNorm2d-9           [-1, 64, 56, 56]             128
17              ReLU-10           [-1, 64, 56, 56]               0
18        BasicBlock-11           [-1, 64, 56, 56]               0
19            Conv2d-12           [-1, 64, 56, 56]          36,864
20       BatchNorm2d-13           [-1, 64, 56, 56]             128
21              ReLU-14           [-1, 64, 56, 56]               0
22            Conv2d-15           [-1, 64, 56, 56]          36,864
23       BatchNorm2d-16           [-1, 64, 56, 56]             128
24              ReLU-17           [-1, 64, 56, 56]               0
25        BasicBlock-18           [-1, 64, 56, 56]               0
26            Conv2d-19          [-1, 128, 28, 28]          73,728
27       BatchNorm2d-20          [-1, 128, 28, 28]             256
28              ReLU-21          [-1, 128, 28, 28]               0
29            Conv2d-22          [-1, 128, 28, 28]         147,456
30       BatchNorm2d-23          [-1, 128, 28, 28]             256
31            Conv2d-24          [-1, 128, 28, 28]           8,192
32       BatchNorm2d-25          [-1, 128, 28, 28]             256
33              ReLU-26          [-1, 128, 28, 28]               0
34        BasicBlock-27          [-1, 128, 28, 28]               0
35            Conv2d-28          [-1, 128, 28, 28]         147,456
36       BatchNorm2d-29          [-1, 128, 28, 28]             256
37              ReLU-30          [-1, 128, 28, 28]               0
38            Conv2d-31          [-1, 128, 28, 28]         147,456
39       BatchNorm2d-32          [-1, 128, 28, 28]             256
40              ReLU-33          [-1, 128, 28, 28]               0
41        BasicBlock-34          [-1, 128, 28, 28]               0
42            Conv2d-35          [-1, 256, 14, 14]         294,912
43       BatchNorm2d-36          [-1, 256, 14, 14]             512
44              ReLU-37          [-1, 256, 14, 14]               0
45            Conv2d-38          [-1, 256, 14, 14]         589,824
46       BatchNorm2d-39          [-1, 256, 14, 14]             512
47            Conv2d-40          [-1, 256, 14, 14]          32,768
48       BatchNorm2d-41          [-1, 256, 14, 14]             512
49              ReLU-42          [-1, 256, 14, 14]               0
50        BasicBlock-43          [-1, 256, 14, 14]               0
51            Conv2d-44          [-1, 256, 14, 14]         589,824
52       BatchNorm2d-45          [-1, 256, 14, 14]             512
53              ReLU-46          [-1, 256, 14, 14]               0
54            Conv2d-47          [-1, 256, 14, 14]         589,824
55       BatchNorm2d-48          [-1, 256, 14, 14]             512
56              ReLU-49          [-1, 256, 14, 14]               0
57        BasicBlock-50          [-1, 256, 14, 14]               0
58            Conv2d-51            [-1, 512, 7, 7]       1,179,648
59       BatchNorm2d-52            [-1, 512, 7, 7]           1,024
60              ReLU-53            [-1, 512, 7, 7]               0
61            Conv2d-54            [-1, 512, 7, 7]       2,359,296
62       BatchNorm2d-55            [-1, 512, 7, 7]           1,024
63            Conv2d-56            [-1, 512, 7, 7]         131,072
64       BatchNorm2d-57            [-1, 512, 7, 7]           1,024
65              ReLU-58            [-1, 512, 7, 7]               0
66        BasicBlock-59            [-1, 512, 7, 7]               0
67            Conv2d-60            [-1, 512, 7, 7]       2,359,296
68       BatchNorm2d-61            [-1, 512, 7, 7]           1,024
69              ReLU-62            [-1, 512, 7, 7]               0
70            Conv2d-63            [-1, 512, 7, 7]       2,359,296
71       BatchNorm2d-64            [-1, 512, 7, 7]           1,024
72              ReLU-65            [-1, 512, 7, 7]               0
73        BasicBlock-66            [-1, 512, 7, 7]               0
74 AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0
75            Linear-68                    [-1, 2]           1,026
76 ================================================================
77 Total params: 11,177,538
78 Trainable params: 11,177,538
79 Non-trainable params: 0
80 ----------------------------------------------------------------
81 Input size (MB): 0.57
82 Forward/backward pass size (MB): 62.79
83 Params size (MB): 42.64
84 Estimated Total Size (MB): 106.00
85 ----------------------------------------------------------------
86 1/83: 1032546534_06907fe3b3.jpg 6.123s 
87 2/83: 10870992_eebeeb3a12.jpg 0.004s 
88 3/83: 1181173278_23c36fac71.jpg 0.003s 
89 4/83: 1297972485_33266a18d9.jpg 0.003s 
90 5/83: 1328423762_f7a88a8451.jpg 0.003s 
91 6/83: 1355974687_1341c1face.jpg 0.003s 
92 7/83: 144098310_a4176fd54d.jpg 0.004s 
93 8/83: 1486120850_490388f84b.jpg 0.003s 
94 9/83: 149973093_da3c446268.jpg 0.004s 
95 10/83: 151594775_ee7dc17b60.jpg 0.005s 
96 11/83: 151603988_2c6f7d14c7.jpg 0.004s 
97 12/83: 1519368889_4270261ee3.jpg 0.004s 
98 13/83: 152789693_220b003452.jpg 0.002s 
99 14/83: 177677657_a38c97e572.jpg 0.006s 
100 15/83: 1799729694_0c40101071.jpg 0.004s 
101 16/83: 181171681_c5a1a82ded.jpg 0.006s 
102 17/83: 187130242_4593a4c610.jpg 0.005s 
103 18/83: 203868383_0fcbb48278.jpg 0.002s 
104 19/83: 2060668999_e11edb10d0.jpg 0.003s 
105 20/83: 2086294791_6f3789d8a6.jpg 0.004s 
106 21/83: 2103637821_8d26ee6b90.jpg 0.002s 
107 22/83: 2104135106_a65eede1de.jpg 0.002s 
108 23/83: 215512424_687e1e0821.jpg 0.003s 
109 24/83: 2173503984_9c6aaaa7e2.jpg 0.002s 
110 25/83: 220376539_20567395d8.jpg 0.002s 
111 26/83: 224841383_d050f5f510.jpg 0.003s 
112 27/83: 2321144482_f3785ba7b2.jpg 0.002s 
113 28/83: 238161922_55fa9a76ae.jpg 0.003s 
114 29/83: 2407809945_fb525ef54d.jpg 0.003s 
115 30/83: 2415414155_1916f03b42.jpg 0.002s 
116 31/83: 2438480600_40a1249879.jpg 0.002s 
117 32/83: 2444778727_4b781ac424.jpg 0.002s 
118 33/83: 2457841282_7867f16639.jpg 0.003s 
119 34/83: 2470492902_3572c90f75.jpg 0.002s 
120 35/83: 2478216347_535c8fe6d7.jpg 0.003s 
121 36/83: 2501530886_e20952b97d.jpg 0.003s 
122 37/83: 2506114833_90a41c5267.jpg 0.002s 
123 38/83: 2509402554_31821cb0b6.jpg 0.003s 
124 39/83: 2525379273_dcb26a516d.jpg 0.003s 
125 40/83: 26589803_5ba7000313.jpg 0.002s 
126 41/83: 2668391343_45e272cd07.jpg 0.002s 
127 42/83: 2670536155_c170f49cd0.jpg 0.005s 
128 43/83: 2685605303_9eed79d59d.jpg 0.004s 
129 44/83: 2702408468_d9ed795f4f.jpg 0.003s 
130 45/83: 2709775832_85b4b50a57.jpg 0.003s 
131 46/83: 2717418782_bd83307d9f.jpg 0.002s 
132 47/83: 272986700_d4d4bf8c4b.jpg 0.004s 
133 48/83: 2741763055_9a7bb00802.jpg 0.004s 
134 49/83: 2745389517_250a397f31.jpg 0.003s 
135 50/83: 2751836205_6f7b5eff30.jpg 0.003s 
136 51/83: 2782079948_8d4e94a826.jpg 0.002s 
137 52/83: 2809496124_5f25b5946a.jpg 0.005s 
138 53/83: 2815838190_0a9889d995.jpg 0.002s 
139 54/83: 2841437312_789699c740.jpg 0.003s 
140 55/83: 2883093452_7e3a1eb53f.jpg 0.003s 
141 56/83: 290082189_f66cb80bfc.jpg 0.002s 
142 57/83: 296565463_d07a7bed96.jpg 0.003s 
143 58/83: 3077452620_548c79fda0.jpg 0.003s 
144 59/83: 348291597_ee836fbb1a.jpg 0.002s 
145 60/83: 350436573_41f4ecb6c8.jpg 0.003s 
146 61/83: 353266603_d3eac7e9a0.jpg 0.002s 
147 62/83: 372228424_16da1f8884.jpg 0.003s 
148 63/83: 400262091_701c00031c.jpg 0.002s 
149 64/83: 416144384_961c326481.jpg 0.002s 
150 65/83: 44105569_16720a960c.jpg 0.002s 
151 66/83: 456097971_860949c4fc.jpg 0.002s 
152 67/83: 464594019_1b24a28bb1.jpg 0.002s 
153 68/83: 485743562_d8cc6b8f73.jpg 0.002s 
154 69/83: 540976476_844950623f.jpg 0.002s 
155 70/83: 54736755_c057723f64.jpg 0.003s 
156 71/83: 57459255_752774f1b2.jpg 0.002s 
157 72/83: 576452297_897023f002.jpg 0.002s 
158 73/83: 586474709_ae436da045.jpg 0.002s 
159 74/83: 590318879_68cf112861.jpg 0.002s 
160 75/83: 59798110_2b6a3c8031.jpg 0.002s 
161 76/83: 603709866_a97c7cfc72.jpg 0.003s 
162 77/83: 603711658_4c8cd2201e.jpg 0.002s 
163 78/83: 65038344_52a45d090d.jpg 0.002s 
164 79/83: 6a00d8341c630a53ef00e553d0beb18834-800wi.jpg 0.002s 
165 80/83: 72100438_73de9f17af.jpg 0.003s 
166 81/83: 759745145_e8bc776ec8.jpg 0.002s 
167 82/83: 936182217_c4caa5222d.jpg 0.002s 
168 83/83: abeja.jpg 0.002s 
169 
170 device:cuda total time:6.4s mean:0.077s

171 GPU name:NVIDIA GeForce RTX 3060
172 [SwanLab-INFO]:        The current experiment resnet_inference_Mar07_10-48-50 has been completed, SwanLab will close it automatically

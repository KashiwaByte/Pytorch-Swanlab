循环神经网络（Recurrent Neural Networks，简称RNN）是一种专门用来处理序列数据的神经网络。与传统的前馈神经网络（如全连接网络和卷积神经网络）不同，RNN能够处理任意长度的序列，因为它们具有在时间上进行递归的能力。RNN在自然语言处理（NLP）、语音识别、时间序列分析等领域有着广泛的应用。

RNN的关键特点是它们在网络层内部具有循环连接，这使得网络可以保持一个内部状态（记忆），从而能够根据前面接收到的信息来影响后续的输出。这种特性使得RNN能够捕捉到序列数据中的时间动态特征。

### RNN的基本结构

RNN的基本单元有一个自环的隐藏层，每个时间步（time step）的隐藏状态 \( h_t \) 是依赖于当前时间步的输入 \( x_t \) 和前一时间步的隐藏状态 \( h_{t-1} \)。数学上，这可以表示为:

\[ h_t = f(W_{hh} \cdot h_{t-1} + W_{xh} \cdot x_t + b_h) \]

其中 \( f \) 是非线性激活函数，如tanh或ReLU。\( W_{hh} \) 和 \( W_{xh} \) 是权重矩阵，\( b_h \) 是偏置项。

最后，RNN可以通过另一组权重将隐藏状态映射到输出 \( y_t \)，这可以表示为：

\[ y_t = W_{hy} \cdot h_t + b_y \]

### RNN的挑战

虽然RNN理论上可以处理长序列中的长期依赖问题，但在实践中，标准的RNN很容易遇到梯度消失和梯度爆炸的问题，这使得网络难以学习到距离较远的依赖关系。为了解决这些问题，研究者们提出了几种改进的RNN结构，包括长短期记忆网络（Long Short-Term Memory，LSTM）和门控循环单元（Gated Recurrent Unit，GRU）。

### LSTM和GRU

LSTM引入了一个复杂的门控机制，包括输入门、忘记门和输出门，来控制信息的流动，这使得它能够更好地捕捉长期依赖关系。GRU是LSTM的一个变体，它简化了门控机制，但仍然能够有效地捕捉长期依赖。

由于其能力在处理序列数据方面非常强大，RNN及其变种被广泛应用于语言模型、文本生成、机器翻译、语音识别等任务中。随着研究的深入，更多的变体和改进方法不断被提出，以优化RNN的性能和稳定性。

处理不定长输入的模型
常用于NLP和时间序列任务

三个操作六个数据
128神经元个数  57向量长度

![image.png](https://kashiwa-pic.oss-cn-beijing.aliyuncs.com/20240307145727.png)

RNN执行会循环计算
四个X就是 Chou，第一个h会初始化一个全0
![image.png](https://kashiwa-pic.oss-cn-beijing.aliyuncs.com/20240307150136.png)

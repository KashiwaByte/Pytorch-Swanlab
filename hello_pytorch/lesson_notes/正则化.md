减小方差的策略
偏差是算法的拟合能力
方差是数据扰动的影响
![image.png](https://kashiwa-pic.oss-cn-beijing.aliyuncs.com/20240306105908.png)

方差是训练集和验证集之间的差异，高方差就是过拟合问题
![image.png](https://kashiwa-pic.oss-cn-beijing.aliyuncs.com/20240306105949.png)


## L1 L2 正则项
L1 绝对值 L2平方
![image.png](https://kashiwa-pic.oss-cn-beijing.aliyuncs.com/20240306110126.png)
同心圆的loss是相同的，此时再通过L1 L2正则项来约束获取具体位置
![image.png](https://kashiwa-pic.oss-cn-beijing.aliyuncs.com/20240306110313.png)

![image.png](https://kashiwa-pic.oss-cn-beijing.aliyuncs.com/20240306110559.png)


# Dropout
随机失活，权值变为0
![image.png](https://kashiwa-pic.oss-cn-beijing.aliyuncs.com/20240306111934.png)

防止太过依赖单一神经元

![image.png](https://kashiwa-pic.oss-cn-beijing.aliyuncs.com/20240306112438.png)

dropout放在要随机失活的网络的前一层
```
  self.linears = nn.Sequential(

  

            nn.Dropout(d_prob),

            nn.Linear(neural_num, 1, bias=False),

            nn.ReLU(inplace=True)

        )
```



## Dropout的eval和train
测试时需要平衡数据尺度
![image.png](https://kashiwa-pic.oss-cn-beijing.aliyuncs.com/20240306113230.png)

在深度学习中，dropout是一种常用的正则化技术，用于防止神经网络过拟合。它通过在训练过程中随机丢弃（即将输出置为零）网络中的一些神经元，从而减少网络的复杂度和内在的依赖关系。这样，网络被迫学习更加鲁棒的特征表示。

在训练和测试过程中，dropout的使用是不同的：

1. 训练时：
   - 在训练过程中，dropout是激活的。也就是说，每次前向传播时，都会按照预设的概率随机选择一部分神经元并将它们的输出置为零。这个概率通常是一个超参数，需要根据具体任务进行调整（常见的值如0.5）。
   - 在反向传播时，只更新那些没有被dropout的权重。

2. 测试时：
   - 在测试（或验证）过程中，dropout是关闭的。这意味着所有的神经元都是激活状态，网络完整无缺。
   - 为了弥补训练时丢弃了一些神经元的影响，通常需要对剩余神经元的输出进行缩放。这个过程称为dropout的“反向缩放”（inverse scaling）或者“权重缩放”（weight scaling）。具体来说，就是将每个神经元的输出乘以保留概率（1减去dropout概率）。例如，如果在训练时dropout概率是0.5，那么在测试时就需要将输出乘以0.5，以保持输出的期望值不变。在许多现代的深度学习框架中，这一缩放过程是自动完成的。

总结来说，dropout在训练时用于防止过拟合，通过随机丢弃神经元来增强网络的泛化能力；而在测试时，为了得到稳定的输出，所有神经元都需要参与计算，且会应用相应的缩放因子来补偿训练时的dropout效果。